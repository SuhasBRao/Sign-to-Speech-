{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sign to Speech Converter (initial notebook)\n",
    "## This Jypyter Notebook shows how the code was built before building the main python script ('application.py').\n",
    "## **Note** : This notebook is recommended to be tested on Google Colab if the required libraries are not installed on\n",
    "## the system. Also the paths for creating folders correspond to the local machine on which this was tested.\n",
    "## **Please ensure to make necessary changes to the paths before executing the cells** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import pyttsx3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import warnings\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import os   # accessing folder paths\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import patoolib  # This is useful to extract zip files \n",
    "import cv2,glob \n",
    "import shutil   # this module is used to do file accessing operations"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "num_classes = {1:'1',2:'2',3:'3', 4:'4', 5:'5', 6:'6', 7:'7', 8:'8',\n",
    "           9:'9'}\n",
    "alpha_classes = {1:'A',2:'B',3:'C',4:'D',5:'E',6:'F',7:'G',8:'H',9:'I',\n",
    "           10:'J',11:'K',12:'L',13:'M',14:'N',15:'O',16:'P',17:'Q',18:'R',19:'S',20:'T',21:'U',\n",
    "           22:'V',23:'W',24:'X',25:'Y',26:'Z'}\n",
    "mylist = [x for x in alpha_classes.values()]\n",
    "print(mylist[8:])\n",
    "words_data = {1:'All_The_Best', 2:'Hi!!', 3: 'I_Love_you', 4: 'No', 5:'Super!!', 6:'Yes'}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create directory for dataset\n",
    "# first create directories inthe name of class keys\n",
    "# You need to run this only once.\n",
    "print('Creating Folders for data. Please wait...')\n",
    "for dir_name in alpha_classes.values():\n",
    "    !mkdir {'G:\\\\gestures\\\\train\\\\' + dir_name}\n",
    "    !mkdir {'G:\\\\gestures\\\\test\\\\' + dir_name}\n",
    "print('Done.')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# creates data for words\n",
    "# RUN THIS ONLY ONCE \n",
    "print('Creating Folders for words data. Please wait....')\n",
    "for dir_name in words_data.values():\n",
    "    !mkdir {'G:\\\\gestures\\\\words_data\\\\test\\\\' + dir_name}\n",
    "    !mkdir {'G:\\\\gestures\\\\words_data\\\\train\\\\' + dir_name}\n",
    "    \n",
    "print('Done!!!')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "background = None\n",
    "accumulated_weight = 0.7\n",
    "mask_color = (0.0,0.0,0.0)\n",
    "\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 300\n",
    "ROI_left = 500"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Below are helper functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# This function is used to calculate accumulated_weights in the frame\n",
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# This function segments the hand region found in the frame, if not found returns None.\n",
    "def segment_hand(frame, threshold=50):\n",
    "    global background\n",
    "    \n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "\n",
    "    \n",
    "    _ , thresholded = cv2.threshold(diff, threshold, 255,cv2.THRESH_BINARY)\n",
    "    \n",
    "    edges = cv2.Canny(thresholded, threshold1= 50, threshold2=250)\n",
    "    cv2.imshow('edges',thresholded)\n",
    "    \n",
    "     #Fetching contours in the frame (These contours can be of hand\n",
    "    #or any other object in foreground) â€¦\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # If length of contours list = 0, means we didn't get any\n",
    "    #contours...\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # The largest external contour should be the hand\n",
    "        # contour_info = [(c, cv2.contourArea(c),) for c in contours[1]]\n",
    "\n",
    "        #cntrs, heirs = cv2.findContours(thresholded.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "        contour_info = [(c, cv2.contourArea(c),) for c in contours]\n",
    "        #for c in contours[1]:\n",
    "        #    contour_info.append((c,cv2.contourArea(c),))\n",
    "        \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # Returning the hand segment(max contour) and the\n",
    "  # thresholded image of hand and contour_info list\n",
    "    return (thresholded, hand_segment_max_cont, contour_info)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "## Initialize tbase dir, train_dir, test_dir\n",
    "#base_dir = 'G:\\\\gestures\\\\words_data\\\\' # un comment this to recognize words\n",
    "base_dir = 'G:\\\\gestures\\\\'            # uncomment this to recognisze numbers\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir,'test')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Here we create the data_set for word recognition\n",
    "## **Note** : Run this cell only when you want to create data\n",
    "### Make sure you have created corresponding directories  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import time\n",
    "for element in alpha_classes.values():\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    num_frames = 0\n",
    "    num_imgs_taken = 0\n",
    "    time.sleep(7)\n",
    "    print('#################################################')\n",
    "    print(f'Show sign for {element}!')\n",
    "\n",
    "    print(f'Creating data for {element}.....')\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cam.read()\n",
    "\n",
    "        # flipping the frame to prevent inverted image of captured frame...\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        frame_copy = frame.copy()\n",
    "\n",
    "        roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "\n",
    "        gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "\n",
    "        if num_frames < 60:\n",
    "            cal_accum_avg(gray_frame, accumulated_weight)\n",
    "            if num_frames <= 59:\n",
    "                cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\",\n",
    "                            (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "                \n",
    "        #Time to configure the hand specifically into the ROI...\n",
    "        elif num_frames <= 300: \n",
    "\n",
    "            hand = segment_hand(gray_frame)\n",
    "            cv2.putText(frame_copy, \"Adjust hand gesture for..\",\n",
    "                            (200, 400), cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 1, (0,0,255), 2)\n",
    "                \n",
    "            if hand is not None:\n",
    "                \n",
    "                thresholded, hand_segment, contour_info = hand\n",
    "\n",
    "                # Draw contours around hand segment\n",
    "                cv2.drawContours(frame_copy, [hand_segment + (ROI_right,\n",
    "                ROI_top)], -1, (255, 0, 0),1)\n",
    "                \n",
    "                cv2.putText(frame_copy, str(num_frames)+\"For\" + str(element),\n",
    "                            (70, 45), cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "                # Also display the thresholded image\n",
    "                cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "        \n",
    "        else: \n",
    "            \n",
    "            # Segmenting the hand region...\n",
    "            hand = segment_hand(gray_frame)\n",
    "            \n",
    "            # Checking if we are able to detect the hand...\n",
    "            if hand is not None:\n",
    "                \n",
    "                # unpack the thresholded img and the max_contour...\n",
    "                thresholded, hand_segment,contour_info = hand\n",
    "\n",
    "                # Drawing contours around hand segment\n",
    "                cv2.drawContours(frame_copy, [hand_segment + (ROI_right,\n",
    "                ROI_top)], -1, (255, 0, 0),1)\n",
    "                \n",
    "                cv2.putText(frame_copy, str(num_frames), (70, 45),cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "                \n",
    "                cv2.putText(frame_copy,\"Adjust hand gesture for..\",(200, 400),cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 1,(0,0,                              255)                                                                                                                   , 2)\n",
    "                # Displaying the thresholded image\n",
    "                cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "                if num_imgs_taken <= 70:\n",
    "                    cv2.imwrite(test_dir + '\\\\' +str(element)+\"\\\\\" + str(num_imgs_taken) + '.jpg',                                   thresholded)\n",
    "                else:\n",
    "                    break\n",
    "                num_imgs_taken +=1\n",
    "            else:\n",
    "                cv2.putText(frame_copy, 'No hand detected...', (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "\n",
    "\n",
    "        # Drawing ROI on frame copy\n",
    "        cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right,ROI_bottom), (255,128,0), 3)\n",
    "        \n",
    "        cv2.putText(frame_copy, \"Sign languge recognition_ _ _\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "        \n",
    "        # increment the number of frames for tracking\n",
    "        num_frames += 1\n",
    "\n",
    "        # Display the frame with segmented hand\n",
    "        cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "\n",
    "        # Closing windows with Esc key...(any other key with ord can be used too.)\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "    # Releasing the camera & destroying all the windows...\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cam.release()\n",
    "    \n",
    "    print('Done!')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cam.release()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "## Initialize tbase dir, train_dir, test_dir\n",
    "#base_dir = 'G:\\\\gestures\\\\words_data\\\\' # un comment this to recognize words\n",
    "base_dir = 'G:\\\\gestures\\\\alpha_data'            # uncomment this to recognisze numbers\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir,'test')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create image data generators for test and train batches"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This cell creates data generators for train and test images.\n",
    "train_batches = ImageDataGenerator(rescale=1./255, rotation_range=40, zoom_range=0.2,\n",
    "      horizontal_flip=True).flow_from_directory(directory=train_dir, target_size=(64,64), class_mode='categorical', batch_size=100,shuffle=True)\n",
    "\n",
    "test_batches = ImageDataGenerator(rescale=1./255, rotation_range=40, zoom_range=0.2,\n",
    "      horizontal_flip=True).flow_from_directory(directory=test_dir, target_size=(64,64), class_mode='categorical', batch_size=10, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot few images to check"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "imgs, labels = next(train_batches)\n",
    "print(words_data)\n",
    "#Plotting the images...\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(50,50))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plotImages(imgs)\n",
    "print('Actual labels:')\n",
    "\n",
    "for i in labels:\n",
    "    print(alpha_classes[np.argmax(i) + 1],end = '  ')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initializing the SEQUENTIAL model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(26,activation =\"softmax\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 31, 31, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                294976    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 413,830\n",
      "Trainable params: 413,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "#early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "#early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BATCH_SIZE = 100\n",
    "epochs=20\n",
    "print(int(np.ceil(train_batches.n/ float(BATCH_SIZE))))\n",
    "\n",
    "history = model.fit(\n",
    "    train_batches,\n",
    "    steps_per_epoch=int(np.ceil(train_batches.n/ float(BATCH_SIZE))),\n",
    "    epochs=epochs,\n",
    "    validation_data=test_batches,\n",
    "    validation_steps=int(np.ceil(test_batches.n/ float(BATCH_SIZE)))\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)\n",
    "#history2 = model.fit(train_batches, epochs=25,  validation_data = test_batches)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Here we print the accuracy of the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# For getting next batch of testing imgs...\n",
    "imgs, labels = next(test_batches)\n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "\n",
    "#Once the model is fitted we save the model using model.save()  function.\n",
    "\n",
    "\n",
    "#model.save('best_model_dataflair3.h5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "acc = history2.history['accuracy']\n",
    "val_acc = history2.history['val_accuracy']\n",
    "\n",
    "loss = history2.history['loss']\n",
    "val_loss = history2.history['val_loss']\n",
    "\n",
    "epochs_range = range(25)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save('alpha_model.h5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction with model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model = keras.models.load_model('new_words_model.h5')\n",
    "background = None\n",
    "text_to_speak = {1:'All The Best', 2:'Hello', 3: 'I Love you', 4: 'No', 5:'Super!!', 6:'Yes'} # used to speak the sign\n",
    "text_to_speak_numbers = num_classes\n",
    "text_to_speak_alpha = alpha_classes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def predict():\n",
    "    \n",
    "    cam = cv2.VideoCapture(0)\n",
    "    num_frames =0\n",
    "    pred = None\n",
    "    while True:\n",
    "        ret, frame = cam.read()\n",
    "\n",
    "        # flipping the frame to prevent inverted image of captured\n",
    "        #frame...\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        frame_copy = frame.copy()\n",
    "\n",
    "        # ROI from the frame\n",
    "        roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "\n",
    "        gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "\n",
    "\n",
    "        if num_frames < 70:\n",
    "            \n",
    "            cal_accum_avg(gray_frame, accumulated_weight)\n",
    "            \n",
    "            cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\",\n",
    "    (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "        \n",
    "        else: \n",
    "            # segmenting the hand region\n",
    "            hand = segment_hand(gray_frame)\n",
    "            \n",
    "            # Checking if we are able to detect the hand...\n",
    "            if hand is not None:\n",
    "                \n",
    "                thresholded, hand_segment,contour_info = hand\n",
    "\n",
    "                # Drawing contours around hand segment\n",
    "                cv2.drawContours(frame_copy, [hand_segment + (ROI_right,\n",
    "        ROI_top)], -1, (255, 0, 0),1)\n",
    "                \n",
    "                cv2.imshow(\"Thesholded Hand Image\", thresholded)\n",
    "                \n",
    "                thresholded = cv2.resize(thresholded, (64, 64))\n",
    "                thresholded = cv2.cvtColor(thresholded,\n",
    "    cv2.COLOR_GRAY2RGB)\n",
    "                thresholded = np.reshape(thresholded,\n",
    "    (1,thresholded.shape[0],thresholded.shape[1],3))\n",
    "\n",
    "                prev = text_to_speak[np.argmax(pred) + 1]\n",
    "                pred = model.predict(thresholded)\n",
    "                #print(pred)\n",
    "                cv2.putText(frame_copy, text_to_speak[np.argmax(pred) + 1],\n",
    "    (300, 45), cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 1, (0,0,255), 2)\n",
    "                \n",
    "                if text_to_speak[np.argmax(pred) + 1] != prev:\n",
    "                    engine = pyttsx3.init()\n",
    "                    engine.say(text_to_speak[np.argmax(pred) + 1])\n",
    "                    engine.runAndWait()\n",
    "                    prev_num_frames = num_frames\n",
    "                        \n",
    "                \n",
    "        # Draw ROI on frame_copy\n",
    "        cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right,\n",
    "        ROI_bottom), (255,128,0), 3)\n",
    "\n",
    "        # incrementing the number of frames for tracking\n",
    "        num_frames += 1\n",
    "        #print(pred)\n",
    "        #if pred != None:\n",
    "            \n",
    "\n",
    "        # Display the frame with segmented hand\n",
    "        cv2.putText(frame_copy, \"Indian sign language recognition_ _ _\",\n",
    "        (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "        cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "\n",
    "\n",
    "        # Close windows with Esc\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "    # Release the camera and destroy all the windows\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "cam.release()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'cam' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-956091a0a570>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cam' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction with tkinter app"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model = keras.models.load_model('new_words_model.h5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import tkinter as tk\n",
    "from PIL import ImageTk, Image\n",
    "from tkinter import filedialog\n",
    "import numpy as np\n",
    "import tensorflow \n",
    "import cv2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def load_img():\n",
    "    global img, image_data\n",
    "    for img_display in frame.winfo_children():\n",
    "        img_display.destroy()\n",
    "\n",
    "    image_data = filedialog.askopenfilename(initialdir=\"/\", title=\"Choose an image\",\n",
    "                                       filetypes=((\"all files\", \"*.*\"), (\"jpg files\", \"*.jpg\")))\n",
    "    basewidth = 150\n",
    "    img = Image.open(image_data)\n",
    "    wpercent = (basewidth / float(img.size[0]))\n",
    "    hsize = int((float(img.size[1]) * float(wpercent)))\n",
    "    img = img.resize((basewidth, hsize), Image.ANTIALIAS)\n",
    "    img = ImageTk.PhotoImage(img)\n",
    "    file_name = image_data.split('/')\n",
    "    panel = tk.Label(frame, text= str(file_name[len(file_name)-1]).upper()).pack()\n",
    "    panel_image = tk.Label(frame, image=img).pack()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "text_to_speak = {1:'All The Best', 2:'Hello', 3: 'I Love you', 4: 'No', 5:'Super!!', 6:'Yes'}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def classify():\n",
    "    original = cv2.imread(image_data)\n",
    "    thresholded = cv2.resize(original, (64, 64))\n",
    "    \n",
    "    thresholded = np.reshape(thresholded,\n",
    "(1,thresholded.shape[0],thresholded.shape[1],3))\n",
    "\n",
    "    pred = model.predict(thresholded)\n",
    "    \n",
    "    string = text_to_speak[np.argmax(pred) + 1]\n",
    "\n",
    "    table = tk.Label(frame, text=\"Predicted sign \").pack()\n",
    "\n",
    "    result = tk.Label(frame, text= string.upper()).pack()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "root = tk.Tk()\n",
    "root.title('Sign predictor')\n",
    "#root.iconbitmap('class.ico')\n",
    "root.resizable(False, False)\n",
    "\n",
    "tit = tk.Label(root, text=\"Sign predictor\", padx=25, pady=6, font=(\"\", 12)).pack()\n",
    "\n",
    "canvas = tk.Canvas(root, height=400, width=600, bg='#76c3fb')\n",
    "canvas.pack()\n",
    "\n",
    "frame = tk.Frame(root, bg='#d776fb')\n",
    "frame.place(relwidth=0.8, relheight=0.7, relx=0.1, rely=0.1)\n",
    "\n",
    "chose_image = tk.Button(root, text='Choose Image',\n",
    "                        padx=20, pady=10,\n",
    "                        fg=\"white\", bg=\"#8c04b5\", command=load_img)\n",
    "chose_image.pack(side=tk.LEFT)\n",
    "\n",
    "live_pred = tk.Button(root, text='Live prediction',\n",
    "                        padx=20, pady=10,\n",
    "                        fg=\"white\", bg=\"#0a7c2e\",command=predict)\n",
    "live_pred.pack(side=tk.RIGHT)\n",
    "\n",
    "\n",
    "\n",
    "class_image = tk.Button(root, text='Classify Image',\n",
    "                            padx=20, pady=10,\n",
    "                            fg=\"white\", bg=\"#0475b5\",command=classify)\n",
    "class_image.pack(side=tk.LEFT)\n",
    "\n",
    " \n",
    "root.mainloop()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\suhas\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\", line 2770, in open\n",
      "    fp.seek(0)\n",
      "AttributeError: 'str' object has no attribute 'seek'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\suhas\\Anaconda3\\lib\\tkinter\\__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-15-aac2fa9a9967>\", line 9, in load_img\n",
      "    img = Image.open(image_data)\n",
      "  File \"C:\\Users\\suhas\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\", line 2772, in open\n",
      "    fp = io.BytesIO(fp.read())\n",
      "AttributeError: 'str' object has no attribute 'read'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "interpreter": {
   "hash": "1aa32c3e39cce6b6a2a7f2e9227e06d8322f53ac0b3d2a2ea2c1a4485a851f90"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}